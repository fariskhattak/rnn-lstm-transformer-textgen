# ğŸ§  Language Modeling with RNN, LSTM, and Transformer

This project implements and compares three types of neural language models â€” Vanilla RNN, LSTM, and Transformer â€” for next-token prediction using a corpus of literary text. The models are trained and evaluated on tokenized inputs with a SentencePiece BPE tokenizer.

---

## ğŸš€ Features
- Token-level language modeling with RNN, LSTM, and Transformer
- SentencePiece tokenizer with BPE encoding
- Evaluation using **Perplexity** and **BLEU score**
- Nucleus sampling (`top-p`) for text generation
- CLI for training, testing, and chatting with models

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                   # JSONL datasets (train.jsonl, val.jsonl, test.jsonl)
â”œâ”€â”€ models/                 # Trained model weights
â”œâ”€â”€ loss_data/              # Trained model loss logs
â”œâ”€â”€ rnn.py                  # RNN model implementation
â”œâ”€â”€ lstm.py                 # LSTM model implementation
â”œâ”€â”€ transformer.py          # Transformer model implementation
â”œâ”€â”€ dataset.py              # TextDataset class for loading and batching
â”œâ”€â”€ main.py                 # Training script
â”œâ”€â”€ test.py                 # Evaluation script
â”œâ”€â”€ chat.py                 # Chat CLI to interact with a trained model
â”œâ”€â”€ README.md               # This file
â””â”€â”€ bpe_tokenizer.model     # Trained SentencePiece tokenizer model
```

---

## ğŸ“¦ Requirements

Install the necessary libraries:
> Youâ€™ll need PyTorch, tqdm, sentencepiece, matplotlib, and nltk.

Also run:
```python
import nltk
nltk.download('punkt')
```

---

## ğŸ”§ Training a Model

```bash
python main.py --model_type rnn         # or lstm or transformer
```
Specifications can be edited within the code of `main.py`

Trained model checkpoints and loss logs will be saved to `models/`.

---

## ğŸ§ª Evaluation

```bash
python test.py
```

This will:
- Load the saved model from `models/`
- Compute **Perplexity** and **BLEU score**

---

## ğŸ’¬ Chat with a Model

You can interact with any **trained model** via CLI by specifying the model type and path to the `.pt` file. You can also optionally customize the architecture using `--embed_dim`, `--hidden_dim`, and `--num_layers`.

### ğŸ” RNN Example
```bash
python chat.py \
  --model_type rnn \
  --model_path models/rnn_e256_h512_l2.pt
  --embed_dim 256 \
  --hidden_dim 512 \
  --num_layers 2
```

### ğŸ” LSTM Example
```bash
python chat.py \
  --model_type lstm \
  --model_path models/lstm_e256_h512_l3.pt \
  --embed_dim 256 \
  --hidden_dim 512 \
  --num_layers 3
```

### ğŸ§  Transformer Example
```bash
python chat.py \
  --model_type transformer \
  --model_path models/transformer_e512_hNone_l6.pt \
  --embed_dim 512 \
  --num_layers 6
```

Example:
```
You: Tell me a story about the moon
Bot: The moon rose slowly over the silent sea, casting silver light on the waves.
```

---

## ğŸ“Š Visualizations

To plot training and validation loss curves:
```bash
python plot_losses.py   # Requires that loss history was saved during training
```
Edit which loss plot to view in the code of `plot_losses.py`

Note: Loss data is based on these models:

1. `transformer_e512_hNone_l6.pt`
2. `lstm_e256_h512_l3.pt`
3. `rnn_e256_h512_l2.pt`

---

## ğŸ“ˆ Results Summary

| Model       | Perplexity | BLEU Score | Best Validation Loss |
|-------------|------------|------------|----------------------|
| RNN         | 134.448    | 0.0001     | 4.9077
| LSTM        | 107.623    | 0.0001     | 4.6908
| Transformer | 82.228     | 0.0003     | 4.4270

---

---

## ğŸ¤– Author
Created by Faris Khattak for CSC 4700 Foundational AI Project 2.
